#!/bin/bash

# this script can run an apptainer sif file of the same name
scriptname=${0##*/}
basename=${scriptname%.*}
if [ -r ${basename}.sif ]; then
    SIF="${basename}.sif"
    APPTAINEROPTS=""
fi

# .. or should be symlinked from a "gputainer" application directory
[ -d ../__gputainer ] || cd ${0%/*}

# !!!!! properly configure VirtualGL !!!!!
# DOC: # https://rawcdn.githack.com/VirtualGL/virtualgl/3.0/doc/index.html#hd006

if [ ! -x /opt/VirtualGL/bin/eglinfo ]; then
    echo "===============================" 1>&2
    echo "VirtualGL is not installed !"    1>&2
    echo "(running on $(hostname))"        1>&2
    echo "===============================" 1>&2
fi

if [ -z "${SLURM_JOB_USER}" ]; then
    echo "==============================="  1>&2
    echo "Notice: not started by SLURM"     1>&2
    echo "(running on $(hostname))"         1>&2
    echo "==============================="  1>&2
fi

if [ -z "${CUDA_VISIBLE_DEVICES}" ]; then
    # SLURM is supposed to set this
    # so we are started by hand
    echo "===============================" 1>&2
    echo "CUDA_VISIBLE_DEVICES IS NOT SET" 1>&2
    echo "(running on $(hostname))"        1>&2
    echo "===============================" 1>&2
    # restricting to the first gpu when not started by slurm
    export CUDA_VISIBLE_DEVICES=0
fi

if [ -z "${dri}" ]; then
    # find the matching /dev/dri device driver
    first_cuda=${CUDA_VISIBLE_DEVICES%%,*}   # 2,3,4,5 => 2

    # this is wrong: (thanks nvidia)  dri=/dev/dri/card${first_cuda}

    # associate nvidia gpu number to matching dri device
    # why the hell is this driver not telling us the information more easily ?
    dri=$(readlink -f /dev/dri/by-path/*$(nvidia-smi --id=${first_cuda} --query-gpu=pci.bus_id --format=csv,noheader | tr '[:upper:]' '[:lower:]' | cut -d: -f2-)*card)
fi

if [ ! -r "${dri}" ]; then
    echo "device '${dri}' is not accessible  ?" 1>&2
    exit 1
fi

echo "CUDA_VISIBLE_DEVICES is set to '${CUDA_VISIBLE_DEVICES}'" 1>&2
echo "DRI card is '${dri}'" 1>&2

vgl_options="-d ${dri}"

# account's HOME might not be accessible at running time
# (on HPC clusters, ${HOME} might be not accessible)
# but there must be one for apptainer:
# using variable TEMPHOME,
# which will be also mapped to /home_local/ and used by some tools
# like blender installer when home is not accessible.
export TEMPHOME=${TEMPHOME-$(pwd)/.temphome}
[ -d ${HOME} ] || export HOME=${TEMPHOME}/$(whoami)

mkdir -p ${HOME}/.vnc

id=${SLURM_JOBID-pid$$}
password1=$$
password2=view  # view-only password
passwordfile=${HOME}/.vnc/${id}-passwd
startup=${HOME}/.vnc/${id}-vncstartup
vncserverlog=${HOME}/.vnc/${id}-vncserverlog.txt
appexecscriptlog=${PWD}/${id}-log.txt
fakehomescript=${HOME}/.vnc/${id}-fakehomescript
appexecscript=${HOME}/.vnc/${id}-appexecscript

if [ -z "${SIF}" ]; then
    dir=${PWD}/generated/${PWD##*/}.sifdir
    if [ -d "${dir}" ]; then
        SIF=${dir}
        APPTAINEROPTS="--writable --bind ${PWD}/generated/PYTHAINER:/pyenv"
    else
        echo "error: cannot find directory '${dir}'" 1>&2
        SIF="unknown-SIF-location"
    fi
fi

rm -f ${vncserverlog} ${appexecscriptlog} ${logstderr} ${passwordfile} ${startup}

############################################################
touch ${fakehomescript}
chmod +x ${fakehomescript}
cat > ${fakehomescript} << EOF
#!/bin/bash

# \$HOME can be an artificial place, and
# - it is not kept up through apptainer
# - it is forbidden to set it up via APPTAINERENV_HOME
# so it is transmitted through APPTAINERENV_MYHOME:
if [ -z "\${MYHOME}" ]; then
    echo "MYHOME (from APPTAINERENV_MYHOME) should be set!"
else
    export HOME=\${MYHOME}
fi
[ -z "\${MYPWD}" ] || cd \${MYPWD}

"\$@"
EOF
############################################################
touch ${appexecscript}
chmod +x ${appexecscript}
cat > ${appexecscript} << EOF
#!/bin/bash -e

if [ -z "\${1}" ]; then
    run=\$(pwd)/run-in-container
else
    run="\${@}"
fi

APPDIR=\${PWD}/generated
APPNAME=\${PWD##*/}

echo APPDIR=\${APPDIR}
echo APPNAME=\${APPNAME}

# passing CUDA_VISIBLE_DEVICES
# set it to 0 (arbitrary) if not currently set
export APPTAINERENV_CUDA_VISIBLE_DEVICES=\${CUDA_VISIBLE_DEVICES-0}

if true; then
    # map /tmp from sub-tmp with linked X11 socket
    tmp=/tmp/apptainer-tmp-\$(date +%y-%m-%d-%H-%M-%S)
    mkdir -p \${tmp}/.X11-unix
    for x in /tmp/.X11-unix/*; do
        [ ! -d "\${x}" ] && ln \${x} \${tmp}/.X11-unix/ || true
    done
else
    # share /tmp and X11 socket
    tmp=/tmp
fi

echo "DRI on host:"
ls -alr /dev/dri
echo "DISPLAY on host: '\${DISPLAY}'"

export APPTAINERENV_MYHOME=\${HOME}
export APPTAINERENV_MYPWD=\${PWD}
export NVIDIA_VISIBLE_DEVICES=all
set -x
time apptainer exec --nv ${APPTAINEROPTS} --bind \$(pwd):/hostpwd --bind /net:/net --bind /local:/local --bind \${tmp}:/tmp --bind ${TEMPHOME}:/home_local ${SIF} ${fakehomescript} \${run}
EOF
############################################################

# this command is run on host inside an xterm and starts the 'appexecscript' which starts apptainer
echo "cd $(pwd); vglrun ${vgl_options} xterm -geometry 160x60 -e bash -c '${appexecscript} \"${@}\" 2>&1 | tee ${appexecscriptlog}'" > ${startup}
chmod +x ${startup}

touch ${passwordfile}
chmod go-rwx ${passwordfile}
( echo ${password1}; echo ${password2}; ) | /opt/TurboVNC/bin/vncpasswd -f >> ${passwordfile}

/opt/TurboVNC/bin/vncserver -vgl -log ${vncserverlog} -rfbauth ${passwordfile} -xstartup ${startup}

waitcounter=10
while [ ! -r ${vncserverlog} ]; do
    waitcounter=$((waitcounter - 1))
    if [ ${waitcounter} -eq 0 ]; then
        echo "Could not start vnc server?" 1>&2
        exit 1
    fi
    echo "waiting for ${vncserverlog} (${waitcounter})"
    sleep 1
done

echo -e "========== LOGFILE: \n${appexecscriptlog}\n"

if true; then

    # dirty hack, see below after 'else'

    sleep 2
    sed -ne "s,.*TurboVNC:.*(\([^)]*\))$,========== RUN  IT NOW ($(date)):\n/opt/TurboVNC/bin/vncviewer -password ${password1} -noreconnect -nonewconn -scale auto \1\n\n========== VIEW IT NOW ($(date)):\n/opt/TurboVNC/bin/vncviewer -password ${password2} -noreconnect -nonewconn -scale auto \1\n,p" ${vncserverlog}
    while true; do
        grep 'Killing Xvnc process ID' ${vncserverlog} && break
        sleep 2
    done
    echo -e "==========\n"

else

    #XXXDEBUGME
    # not working with slurm ?
    # but working when started by hand
    #   (debug so far: 'tail file | sed -ne' is always empty)

    tail -n +0 -F ${vncserverlog} | sed -ne "s,.*TurboVNC:.*(\([^)]*\))$,========== RUN  IT NOW ($(date)):\n/opt/TurboVNC/bin/vncviewer -password ${password1} -noreconnect -nonewconn -scale auto \1\n\n========== VIEW IT NOW ($(date)):\n/opt/TurboVNC/bin/vncviewer -password ${password2} -noreconnect -nonewconn -scale auto \1\n,p" & shower=$!
    tail -n +0 -F ${vncserverlog} | sed -ne "/Killing Xvnc process ID/q"
    echo -e "==========\n"
    kill ${shower}

fi
